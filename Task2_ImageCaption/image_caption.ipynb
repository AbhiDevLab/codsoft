{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow numpy pillow\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "MAX_CAPTION_LENGTH = 100\n",
    "VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 128\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "!rm -rf images\n",
    "!rm -rf models\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def upload_file(prompt, required_name):\n",
    "    print(prompt)\n",
    "    uploaded = files.upload()\n",
    "    while required_name not in uploaded:\n",
    "        print(f\"Error: File must be named exactly '{required_name}'\")\n",
    "        print(\"Please upload again:\")\n",
    "        uploaded = files.upload()\n",
    "    return uploaded\n",
    "\n",
    "upload_file(\"1. Please upload your captions.txt file:\", \"captions.txt\")\n",
    "upload_file(\"\\n2. Please upload your images (as ZIP file):\", \"images.zip\")\n",
    "\n",
    "print(\"\\nUnzipping images...\")\n",
    "!unzip -o images.zip -d images/\n",
    "print(\"\\nFiles in images directory:\", os.listdir('images'))\n",
    "\n",
    "BASE_DIR = '/content'\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images\", \"images\")\n",
    "CAPTION_FILE = os.path.join(BASE_DIR, \"captions.txt\")\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, \"models\", \"image_captioning_model.h5\")\n",
    "TOKENIZER_SAVE_PATH = os.path.join(BASE_DIR, \"models\", \"tokenizer.pkl\")\n",
    "\n",
    "def load_captions(caption_file):\n",
    "    captions = {}\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            img_id, caption = parts\n",
    "            captions.setdefault(img_id.strip(), []).append('startseq ' + caption.strip().lower() + ' endseq')\n",
    "    return captions\n",
    "\n",
    "def extract_features(image_dir):\n",
    "    model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "        try:\n",
    "            img_path = os.path.join(image_dir, img_name)\n",
    "            img = Image.open(img_path).convert('RGB').resize((224, 224))\n",
    "            img_array = img_to_array(img) / 255.0\n",
    "            features[os.path.splitext(img_name)[0]] = model.predict(np.expand_dims(img_array, axis=0), verbose=0).flatten()\n",
    "        except:\n",
    "            continue\n",
    "    return features\n",
    "\n",
    "def prepare_all_data(captions, features, tokenizer):\n",
    "    X1, X2, y = [], [], []\n",
    "    for img_id, caps in captions.items():\n",
    "        if img_id not in features:\n",
    "            continue\n",
    "        for cap in caps:\n",
    "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                X1.append(features[img_id])\n",
    "                X2.append(pad_sequences([seq[:i]], maxlen=MAX_CAPTION_LENGTH)[0])\n",
    "                y.append(to_categorical(seq[i], num_classes=vocab_size))\n",
    "    return [np.array(X1), np.array(X2)], np.array(y)\n",
    "\n",
    "def build_model():\n",
    "    input1 = Input(shape=(512,))\n",
    "    fe1 = Dense(128, activation='relu')(input1)\n",
    "\n",
    "    input2 = Input(shape=(MAX_CAPTION_LENGTH,))\n",
    "    se1 = Embedding(vocab_size, EMBEDDING_DIM)(input2)\n",
    "    se2 = LSTM(LSTM_UNITS)(se1)\n",
    "\n",
    "    decoder1 = add([fe1, se2])\n",
    "    decoder2 = Dense(128, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_caption_beam_search(model, tokenizer, photo_feature, max_length=30, beam_index=3):\n",
    "    start = [[[tokenizer.word_index['startseq']], 0.0]]\n",
    "    while len(start[0][0]) < max_length:\n",
    "        temp = []\n",
    "        for s in start:\n",
    "            sequence = pad_sequences([s[0]], maxlen=max_length)\n",
    "            preds = model.predict([photo_feature, sequence], verbose=0)\n",
    "            top_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            for w in top_preds:\n",
    "                next_seq, prob = s[0][:], s[1]\n",
    "                next_seq.append(w)\n",
    "                prob += np.log(preds[0][w] + 1e-10)\n",
    "                temp.append([next_seq, prob])\n",
    "        start = sorted(temp, reverse=False, key=lambda l: l[1])[-beam_index:]\n",
    "    final_seq = start[-1][0]\n",
    "    caption = [tokenizer.index_word.get(i, '') for i in final_seq]\n",
    "    caption = ' '.join(caption)\n",
    "    caption = caption.replace('startseq', '').strip()\n",
    "    if 'endseq' in caption:\n",
    "        caption = caption[:caption.index('endseq')].strip()\n",
    "    return caption\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    print(\"\\n=== STARTING PIPELINE ===\")\n",
    "\n",
    "    captions = load_captions(CAPTION_FILE)\n",
    "    features = extract_features(IMAGE_DIR)\n",
    "\n",
    "    all_captions = [cap for caps in captions.values() for cap in caps]\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\", filters='')\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    X_train, y_train = prepare_all_data(captions, features, tokenizer)\n",
    "\n",
    "    actual_batch_size = min(BATCH_SIZE, len(y_train))\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train, batch_size=actual_batch_size, epochs=EPOCHS, verbose=2, validation_split=0.2)\n",
    "\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "    with open(TOKENIZER_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(\"\\n=== TESTING ===\")\n",
    "    test_images = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if test_images:\n",
    "        test_image = os.path.join(IMAGE_DIR, test_images[0])\n",
    "        print(f\"Testing on: {test_images[0]}\")\n",
    "        img = Image.open(test_image).convert('RGB').resize((224, 224))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        photo_feature = features[os.path.splitext(test_images[0])[0]].reshape((1, 512))\n",
    "\n",
    "        caption = generate_caption_beam_search(model, tokenizer, photo_feature, max_length=MAX_CAPTION_LENGTH, beam_index=5)\n",
    "        print(\"\\nGenerated Caption:\", caption)\n",
    "    else:\n",
    "        print(\"No test images available\")\n",
    "\n",
    "    print(\"\\n=== PROCESS COMPLETED ===\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
